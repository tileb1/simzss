import math
import warnings

import torch
import torch.nn.functional as F
from einops import rearrange
from torch import nn


class MMAttention(nn.Module):
    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):
        super().__init__()
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = qk_scale or head_dim ** -0.5

        self.weights_q = nn.Linear(dim, dim, bias=qkv_bias)
        self.weights_k = nn.Linear(dim, dim, bias=qkv_bias)
        self.weights_v = nn.Linear(dim, dim, bias=qkv_bias)

        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)

    def forward(self, q, k, v):
        # Get head-wise representations
        q = rearrange(self.weights_q(q), 'b l (h d) -> b l h d', h=self.num_heads)
        k = rearrange(self.weights_k(k), 'n (h d) -> n h d', h=self.num_heads)
        v = rearrange(self.weights_v(v), 'n (h d) -> n h d', h=self.num_heads)

        # Compute the self-attention
        attn = torch.einsum('b l h d, n h d -> b l h n', q, k) * self.scale
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)

        # Update the values
        v = torch.einsum('b l h n, n h d -> b l h d', attn, v)
        v = rearrange(v, 'b l h d -> b l (h d)')
        v = self.proj(v)
        v = self.proj_drop(v)
        return v, attn


class CLIPHead(nn.Module):
    def __init__(self, layer_norm, projection, all_classes_embeddings, all_classes_names, classes_embeddings,
                 classes_names, num_heads=1, lambda_modality=0.1):
        super().__init__()
        self.layer_norm = layer_norm
        self.projection = projection
        self.words = all_classes_names
        self.class_names = classes_names

        # Set the last layer with normalized text embeddings
        self.all_classes_embeddings = all_classes_embeddings
        self.all_classes_embeddings_normed = F.normalize(all_classes_embeddings, p=2, dim=-1)

        # Store the class embeddings
        self.classes_embeddings_normed = F.normalize(classes_embeddings, p=2, dim=-1)

        # Attention layer
        self.attention_layer = MMAttention(
            dim=all_classes_embeddings.shape[-1],
            num_heads=num_heads,
        )
        self.lambda_modality = lambda_modality

    def forward(self, x):
        spatial_tokens = None
        if isinstance(x, tuple):
            x, spatial_tokens = x
            spatial_tokens = self.layer_norm(spatial_tokens)
            spatial_tokens = spatial_tokens @ self.projection

        # Project to same space as text
        x = self.layer_norm(x)
        x = x @ self.projection

        # Compute the predictions
        if spatial_tokens is not None:
            with torch.no_grad():
                class_predictions = spatial_tokens @ self.classes_embeddings_normed.T
        else:
            with torch.no_grad():
                class_predictions = x @ self.classes_embeddings_normed.T

        # Cross-modality attention (is all you need)
        queries = nn.functional.normalize(x, dim=-1, p=2)
        keys = self.all_classes_embeddings_normed
        values = self.all_classes_embeddings
        attended_x, _ = self.attention_layer(queries, keys, values)
        x = nn.functional.normalize(x, dim=-1, p=2)
        attended_x = nn.functional.normalize(attended_x, dim=-1, p=2)
        x = (1. - self.lambda_modality) * x + self.lambda_modality * attended_x

        # DINO head
        x = self.mlp(x)
        x = nn.functional.normalize(x, dim=-1, p=2)
        x = self.last_layer(x)
        return x, class_predictions


def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)
